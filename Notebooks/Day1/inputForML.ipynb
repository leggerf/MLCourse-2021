{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Day 1\n",
    "\n",
    "- course [slides](https://github.com/leggerf/MLCourse-2021/blob/master/Slides/Day1/Big%20data%20science%20-%20Day%201%20-%202021.pdf)\n",
    "\n",
    "# You'll learn\n",
    "\n",
    "- familiarize with jupyter notebooks, numpy and pandas\n",
    "\n",
    "## Input data\n",
    "- efficient data format: convert CSV to Parquet\n",
    "- create input vector with features for MLLib. Format of the input depends on chosen ML library\n",
    "\n",
    "## Visualization\n",
    "- explore dataset, plot features\n",
    "- correlation matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset description\n",
    "\n",
    "The dataset used in this example is described [here](https://archive.ics.uci.edu/ml/datasets/HIGGS). It is a binary classification problem where the goal is to train a classifier able to distinguish between a signal process, the production of new theoretical Higgs bosons, and a background process with identical decay products but distinct kinematic features.\n",
    "\n",
    "Each row of this dataset contains 28 features plus the label:\n",
    "\n",
    "- 21 low-level features which represent the basic measure made by the particle detector\n",
    " -        Momentum of the observed paricles\n",
    " -        Missing transverse momentum\n",
    " -        Jets and b-tagging information\n",
    "- 7 high-level features computed from the low-level features that encode the knowledge of the different intermediate states of the two processes (reconstructed invariant masses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the execution environment\n",
    "\n",
    "Your code will run on a single dedicated server with 24 cores (hyperthreading enabled) and 192 GB of RAM. \n",
    "All the services needed for this tutorial are deployed as Kubernetes applications on this server. These include:\n",
    "* JupytherHub\n",
    "* Jupyter single-user servers\n",
    "* the HDFS file-system\n",
    "* Spark Clusters on demand "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load custom magics definition\n",
    "\n",
    "We load an external file implemanting some custom *magics* function. Have a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext custom_magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Check out these custom functions\n",
    "from custom_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Spark context\n",
    "\n",
    "We use the custom magic *%sc* to load a pre-defined Spark context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-leggerf.jhub.svc.cluster.local:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>k8s://https://192.168.2.39:6443</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>jupyter-leggerf</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=k8s://https://192.168.2.39:6443 appName=jupyter-leggerf>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_workers=5\n",
    "spark=%sc $num_workers\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark_session = SparkSession(spark)\n",
    "\n",
    "#check if spark is there\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "## Get familiar with kubernetes commands\n",
    "\n",
    "- You can open a terminal and use the commands: \n",
    "    `kubectl get pods`\n",
    "    `kubectl describe pod PODNAME`\n",
    "    `kubectl get nodes`\n",
    "    `kubectl describe node NODENAME`\n",
    "    `kubectl describe farm`\n",
    "to see what's happening there. \n",
    "\n",
    "- Try to stop spark with `spark.stop()`, and start it again running the cell above with a different number of workers. What happens? You can play with the number of workers, and run the cells below that execute spark commands. Provided you're getting all the workers you're asking for, does the execution time change? Try to make some scaling tests\n",
    "\n",
    "- if you don't stop spark correctly, you will see pods in Error state. You can get rid of those by running this command:\n",
    "    `kubectl get pods -n YOURUSERNAME | grep Error | awk '{print $1}' | xargs kubectl delete pod -n YOURUSERNAME`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create parquet files, which are faster to read than CSV\n",
    "\n",
    "Read [here](https://operational-intelligence.web.cern.ch/DataFormats) for a description of the most common data formats and their use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 73.5 ms, sys: 25.7 ms, total: 99.2 ms\n",
      "Wall time: 18.2 s\n"
     ]
    }
   ],
   "source": [
    "# read from HDFS\n",
    "inputFile = 'hdfs://192.168.2.39/data/Higgs1M.csv'\n",
    "\n",
    "%time df = spark_session.read.format('csv').option('header', 'true').option('inferschema', 'true').load(inputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write parquet\n",
    "hdfs_path=%hdfs_path\n",
    "outputFile = hdfs_path+'/Higgs1M.parquet'\n",
    "\n",
    "df.write.parquet(outputFile, mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in parquet files\n",
    "\n",
    "how much faster is it to read parquet files rather than csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.92 ms, sys: 2.91 ms, total: 11.8 ms\n",
      "Wall time: 937 ms\n"
     ]
    }
   ],
   "source": [
    "inputFile = 'hdfs://192.168.2.39/data/Higgs10M.parquet'\n",
    "#inputFile = 'hdfs://192.168.2.39/data/Higgs1M.parquet'\n",
    "#inputFile = 'hdfs://192.168.2.39/data/Higgs100k.parquet'\n",
    "#other files are: Higgs1M.parquet, Higgs100k.parquet\n",
    "\n",
    "%time df = spark_session.read.format('parquet').option('header', 'true').option('inferschema', 'true').load(inputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 26\n",
      "Partitioner: None\n"
     ]
    }
   ],
   "source": [
    "#In how many partitions is the dataframe distributed?\n",
    "print(\"Number of partitions: {}\".format(df.rdd.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(df.rdd.partitioner))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "## Get familiar with spark workers\n",
    "\n",
    "- Try to stop spark with `spark.stop()`, and start it again running the cell above with a different number of workers. What happens when you run `kubectl get pods`? \n",
    "- You can play with the number of workers, and run any of the cells that execute spark commands. Provided you're getting all the workers you're asking for, does the execution time change? Try to make some scaling tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's have a look at the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.8 ms, sys: 16 ms, total: 73.7 ms\n",
      "Wall time: 1.81 s\n",
      "There are 9560096 events\n"
     ]
    }
   ],
   "source": [
    "%time total_events = df.count()\n",
    "\n",
    "print('There are '+str(total_events)+' events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- lepton_pT: double (nullable = true)\n",
      " |-- lepton_eta: double (nullable = true)\n",
      " |-- lepton_phi: double (nullable = true)\n",
      " |-- missing_energy_magnitude: double (nullable = true)\n",
      " |-- missing_energy_phi: double (nullable = true)\n",
      " |-- jet1_pt: double (nullable = true)\n",
      " |-- jet1_eta: double (nullable = true)\n",
      " |-- jet1_phi: double (nullable = true)\n",
      " |-- jet1_b-tag: double (nullable = true)\n",
      " |-- jet2_pt: double (nullable = true)\n",
      " |-- jet2_eta: double (nullable = true)\n",
      " |-- jet2_phi: double (nullable = true)\n",
      " |-- jet2_b-tag: double (nullable = true)\n",
      " |-- jet3_pt: double (nullable = true)\n",
      " |-- jet3_eta: double (nullable = true)\n",
      " |-- jet3_phi: double (nullable = true)\n",
      " |-- jet3_b-tag: double (nullable = true)\n",
      " |-- jet4_pt: double (nullable = true)\n",
      " |-- je4_eta: double (nullable = true)\n",
      " |-- jet4_phi: double (nullable = true)\n",
      " |-- jet4_b-tag: double (nullable = true)\n",
      " |-- m_jj: double (nullable = true)\n",
      " |-- m_jjj: double (nullable = true)\n",
      " |-- m_lv: double (nullable = true)\n",
      " |-- m_jlv: double (nullable = true)\n",
      " |-- m_bb: double (nullable = true)\n",
      " |-- m_wbb: double (nullable = true)\n",
      " |-- m_wwbb: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "- Create a function that makes a plot of any of the above variable for signal versus background (using the label variable to discriminate)\n",
    "  - see an example of the plot in the hands-on [slides](https://github.com/leggerf/MLCourse-2021/blob/master/Slides/Day1/ML-handson-day1.pdf)\n",
    "  - the function should take as input the dataframe *df* and the variable name. For example `plotSignalvsBg(df, 'm_bb')`\n",
    "  - to develop the code, use the 100k dataset, so that debugging goes quicker\n",
    "- try to plot a few input variables and try to understand which ones are more promising to distinguish signal from background  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 - Bonus\n",
    "\n",
    "#### Create the input feature vector\n",
    "\n",
    "- Libraries for ML tipically take as inputs data in a very specific format. Documentation on how to do data preprocessing in Spark: https://spark.apache.org/docs/latest/ml-features.html\n",
    "- Try to add to the dataframe df a new column, called 'features' which is a vector column with all the variables above except for 'label'\n",
    "   - features = [lepton_pT, lepton_eta, lepton_phi, ...]\n",
    "   - Hint: look at the VectorAssembler transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print (or draw) the correlation matrix (a table showing correlation coefficients between variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when you're done, stop spark, this will release the resources you're using\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "sparkconnect": {
   "bundled_options": [],
   "list_of_options": [
    {
     "name": "spark.driver.maxResultSize",
     "value": "0"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
